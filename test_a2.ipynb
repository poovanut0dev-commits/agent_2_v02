{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac1a05c-4f69-4205-952b-85ac47939dbd",
   "metadata": {},
   "source": [
    "# Agent #2: The Librarian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8760d6-9a38-4c36-b417-a8ba2bff3ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent #2: The Librarian\n",
      "\n",
      "CHARACTER COUNT REPORT\n",
      "--------------------------------------------------\n",
      "PDF (raw text)      : 138,990 chars\n",
      "Markdown (.md)      : 141,236 chars\n",
      "Metadata (.jsonl)   : 132,285 chars\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████████████████████████████████████████████████████████████| 220/220 [05:19<00:00,  1.45s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱ Data preparation time : 0.91 sec\n",
      "⏱ Embedding + Index time: 319.66 sec\n",
      "\n",
      "Agent Ready (type 'exit')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  นอกเหนือจากหน่วยงานภายใน มธ. แล้ว หลักสูตร DSI (พ.ศ. 2566) ยังร่วมมือกับบริษัทเอกชนใดบ้าง?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/28 09:57:35 WARNING dspy.primitives.module: Failed to set LM usage. Please return `dspy.Prediction` object from dspy.Module to enable usage tracking.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " นอกจากหน่วยงานภายในมหาวิทยาลัยธรรมศาสตร์แล้ว หลักสูตร DSI (พ.ศ. 2566) ยังได้รับความร่วมมือจากบริษัทเอกชนสองแห่งคือ  \n",
      "\n",
      "* **บริษัท อี‑ซี‑โอ‑พี (ประเทศไทย) จำกัด**  \n",
      "* **บริษัท กสิกร แล็ป จำกัด**  \n",
      "\n",
      "(ข้อมูลอ้างอิงจาก [หน้า 3]​)\n",
      "⏱ Query processing time : 10.22 sec\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  หลักสูตรวิทยาศาสตร์และนวัตกรรมข้อมูล พ.ศ. 2566 เป็นการปรับปรุงจากหลักสูตรเดิมในปี พ.ศ. ใด?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/28 09:58:12 WARNING dspy.primitives.module: Failed to set LM usage. Please return `dspy.Prediction` object from dspy.Module to enable usage tracking.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " หลักสูตรวิทยาศาสตร์และนวัตกรรมข้อมูล พ.ศ. 2566 ได้รับการปรับปรุงจากหลักสูตรเดิมในปี พ.ศ. 2561 (ระบุในหน้า 3)\n",
      "⏱ Query processing time : 7.56 sec\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  นักศึกษาหลักสูตร DSI (พ.ศ. 2566) ต้องศึกษาทั้งหมดกี่หน่วยกิตจึงจะสำเร็จการศึกษา?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/28 09:59:08 WARNING dspy.primitives.module: Failed to set LM usage. Please return `dspy.Prediction` object from dspy.Module to enable usage tracking.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " นักศึกษาหลักสูตร DSI (พ.ศ. 2566) ต้องสะสมหน่วยกิตอย่างน้อย **100 หน่วยกิต** จึงจะสามารถสำเร็จการศึกษาได้  \n",
      "(รวม 12 หน่วยกิต วิชาแกนร่วมคณะ + 21 หน่วยกิต วิชาพื้นฐาน + 36 หน่วยกิต วิชาบังคับในสาขา + 12 หน่วยกิต วิชาเลือกในสาขา + 1 หน่วยกิต ฝึกปฏิบัติงาน + 12 หน่วยกิต โครงงาน‑สหกิจศึกษา + 6 หน่วยกิต วิชาเลือกเสรี) [หน้า 11]​, [หน้า 14]​.\n",
      "⏱ Query processing time : 11.10 sec\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ADVANCED MULTI-HOP AGENTIC PDF QA (DSPy SAFE)\n",
    "# + Markdown Export\n",
    "# + Character Count Report (COUNT ONLY)\n",
    "# + Runtime Measurement\n",
    "# ============================================================\n",
    "\n",
    "import fitz\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "import requests\n",
    "import numpy as np\n",
    "import faiss\n",
    "import dspy\n",
    "import time\n",
    "\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "PDF_PATH = \"doc/dsi2566.pdf\"\n",
    "\n",
    "WORKDIR = Path(\"./rag_store\")\n",
    "WORKDIR.mkdir(exist_ok=True)\n",
    "\n",
    "PDF_NAME = Path(PDF_PATH).stem\n",
    "\n",
    "MD_PATH    = WORKDIR / f\"{PDF_NAME}_document.md\"\n",
    "META_PATH  = WORKDIR / f\"{PDF_NAME}_chunks.jsonl\"\n",
    "EMB_PATH   = WORKDIR / f\"{PDF_NAME}_embeddings.npy\"\n",
    "INDEX_PATH = WORKDIR / f\"{PDF_NAME}_faiss.index\"\n",
    "\n",
    "EMBEDDING_API = \"http://localhost:11434/api/embeddings\"\n",
    "EMBED_MODEL   = \"bge-m3:567m\"\n",
    "\n",
    "LLM_MODEL = \"gpt-oss:120b-cloud\"\n",
    "\n",
    "BASE_TOP_K   = 3\n",
    "EXPAND_TOP_K = 6\n",
    "SCORE_TH     = 0.15\n",
    "\n",
    "# ============================================================\n",
    "# DSPy LLM (RAW TEXT ONLY)\n",
    "# ============================================================\n",
    "\n",
    "class OllamaDSPyLM(dspy.LM):\n",
    "    def __init__(self, model: str):\n",
    "        super().__init__(model=model)\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, prompt=None, messages=None, **kwargs):\n",
    "        from ollama import chat\n",
    "        if messages is None:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        r = chat(model=self.model, messages=messages)\n",
    "        return r.message.content\n",
    "\n",
    "dspy.configure(lm=OllamaDSPyLM(LLM_MODEL), track_usage=True)\n",
    "\n",
    "# ============================================================\n",
    "# UTILS\n",
    "# ============================================================\n",
    "\n",
    "def sha1(text: str) -> str:\n",
    "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def normalize_query(q: str) -> str:\n",
    "    if not q:\n",
    "        return \"\"\n",
    "    q = q.strip()\n",
    "    q = re.sub(r\"^[\\-\\*\\•\\d\\.\\s]+\", \"\", q)\n",
    "    return q.strip()\n",
    "\n",
    "# ---------- character counting ----------\n",
    "\n",
    "def count_pdf_chars(path: str) -> int:\n",
    "    doc = fitz.open(path)\n",
    "    total = 0\n",
    "    for p in doc:\n",
    "        t = p.get_text(\"text\")\n",
    "        if t:\n",
    "            total += len(t)\n",
    "    doc.close()\n",
    "    return total\n",
    "\n",
    "def count_md_chars(path: Path) -> int:\n",
    "    if not path.exists():\n",
    "        return 0\n",
    "    return len(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def count_jsonl_chars(path: Path) -> int:\n",
    "    if not path.exists():\n",
    "        return 0\n",
    "    total = 0\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            total += len(obj.get(\"text\", \"\"))\n",
    "    return total\n",
    "\n",
    "# ============================================================\n",
    "# 1) LOAD PDF\n",
    "# ============================================================\n",
    "\n",
    "def load_pdf_pages(path: str):\n",
    "    doc = fitz.open(path)\n",
    "    pages = []\n",
    "\n",
    "    for i, page in enumerate(doc):\n",
    "        text = page.get_text(\"text\")\n",
    "        if text and text.strip():\n",
    "            pages.append({\"page\": i + 1, \"text\": text})\n",
    "\n",
    "    doc.close()\n",
    "    return pages\n",
    "\n",
    "# ============================================================\n",
    "# 2) PAGE → CHUNKS\n",
    "# ============================================================\n",
    "\n",
    "def page_to_chunks(page: Dict) -> List[Dict]:\n",
    "    page_num = page[\"page\"]\n",
    "    lines = page[\"text\"].splitlines()\n",
    "\n",
    "    chunks = []\n",
    "    section = \"(no section)\"\n",
    "    buffer = []\n",
    "\n",
    "    def flush():\n",
    "        nonlocal buffer\n",
    "        if buffer:\n",
    "            text = \"\\n\".join(buffer).strip()\n",
    "            chunks.append({\n",
    "                \"id\": sha1(text),\n",
    "                \"page\": page_num,\n",
    "                \"section\": section,\n",
    "                \"text\": text\n",
    "            })\n",
    "            buffer = []\n",
    "\n",
    "    for line in lines:\n",
    "        l = line.strip()\n",
    "        if not l:\n",
    "            continue\n",
    "\n",
    "        if re.match(r\"^(บทที่|Chapter)\\s+\\d+\", l) or (len(l) < 60 and l.isupper()):\n",
    "            flush()\n",
    "            section = l\n",
    "            buffer.append(f\"## {l}\")\n",
    "            continue\n",
    "\n",
    "        buffer.append(l)\n",
    "\n",
    "    flush()\n",
    "    return chunks\n",
    "\n",
    "# ============================================================\n",
    "# 3) BUILD CHUNKS\n",
    "# ============================================================\n",
    "\n",
    "def build_chunks() -> List[Dict]:\n",
    "    pages = load_pdf_pages(PDF_PATH)\n",
    "    chunks = []\n",
    "    for p in pages:\n",
    "        chunks.extend(page_to_chunks(p))\n",
    "    return chunks\n",
    "\n",
    "# ============================================================\n",
    "# 4) SAVE / LOAD FILES\n",
    "# ============================================================\n",
    "\n",
    "def save_chunks_jsonl(chunks: List[Dict]):\n",
    "    with open(META_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for c in chunks:\n",
    "            f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def save_markdown(chunks: List[Dict]):\n",
    "    with open(MD_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for c in chunks:\n",
    "            f.write(\n",
    "                f\"<!-- page:{c['page']} section:{c['section']} -->\\n\"\n",
    "                f\"{c['text']}\\n\\n\"\n",
    "            )\n",
    "\n",
    "def load_chunks() -> List[Dict]:\n",
    "    with open(META_PATH, encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "# ============================================================\n",
    "# 5) CHARACTER COUNT REPORT\n",
    "# ============================================================\n",
    "\n",
    "def report_char_count(pdf_chars: int, md_chars: int, json_chars: int):\n",
    "    print(\"\\nCHARACTER COUNT REPORT\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"PDF (raw text)      : {pdf_chars:,} chars\")\n",
    "    print(f\"Markdown (.md)      : {md_chars:,} chars\")\n",
    "    print(f\"Metadata (.jsonl)   : {json_chars:,} chars\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# ============================================================\n",
    "# 6) EMBEDDING + FAISS\n",
    "# ============================================================\n",
    "\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    vectors = []\n",
    "    for t in tqdm(texts, desc=\"Embedding\", unit=\"chunk\"):\n",
    "        r = requests.post(\n",
    "            EMBEDDING_API,\n",
    "            json={\"model\": EMBED_MODEL, \"prompt\": t},\n",
    "            timeout=120\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        vectors.append(r.json()[\"embedding\"])\n",
    "    return np.array(vectors, dtype=\"float32\")\n",
    "\n",
    "def embed_query(query: str, dim: int):\n",
    "    query = normalize_query(query)\n",
    "    if not query:\n",
    "        return None\n",
    "\n",
    "    r = requests.post(\n",
    "        EMBEDDING_API,\n",
    "        json={\"model\": EMBED_MODEL, \"prompt\": query},\n",
    "        timeout=60\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "\n",
    "    vec = np.array([r.json()[\"embedding\"]], dtype=\"float32\")\n",
    "    if vec.shape[1] != dim:\n",
    "        return None\n",
    "\n",
    "    faiss.normalize_L2(vec)\n",
    "    return vec\n",
    "\n",
    "def build_or_load_embeddings(chunks):\n",
    "    if EMB_PATH.exists():\n",
    "        return np.load(EMB_PATH)\n",
    "\n",
    "    emb = embed_texts([c[\"text\"] for c in chunks])\n",
    "    faiss.normalize_L2(emb)\n",
    "    np.save(EMB_PATH, emb)\n",
    "    return emb\n",
    "\n",
    "def build_or_load_index(embeddings):\n",
    "    if INDEX_PATH.exists():\n",
    "        return faiss.read_index(str(INDEX_PATH))\n",
    "\n",
    "    index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, str(INDEX_PATH))\n",
    "    return index\n",
    "\n",
    "# ============================================================\n",
    "# 7) RETRIEVAL\n",
    "# ============================================================\n",
    "\n",
    "def retrieve(query: str, chunks, index, top_k):\n",
    "    q_emb = embed_query(query, index.d)\n",
    "    if q_emb is None:\n",
    "        return []\n",
    "\n",
    "    scores, idxs = index.search(q_emb, top_k)\n",
    "    return [chunks[i] for s, i in zip(scores[0], idxs[0]) if s >= SCORE_TH]\n",
    "\n",
    "# ============================================================\n",
    "# 8) MULTI-HOP AGENTS\n",
    "# ============================================================\n",
    "\n",
    "class QueryExpansionAgent(dspy.Module):\n",
    "    def forward(self, question: str):\n",
    "        prompt = f\"\"\"\n",
    "แตกคำถามต่อไปนี้เป็นคำถามย่อย 2-3 ข้อ\n",
    "ตอบเป็นบรรทัดสั้น ๆ เท่านั้น\n",
    "\n",
    "[Question]\n",
    "{question}\n",
    "\"\"\"\n",
    "        text = dspy.settings.lm(prompt)\n",
    "        return [normalize_query(l) for l in text.splitlines() if len(normalize_query(l)) > 3]\n",
    "\n",
    "class DocumentQAAgent(dspy.Module):\n",
    "    def forward(self, context: str, question: str):\n",
    "        prompt = f\"\"\"\n",
    "คุณคือ AI สำหรับตอบคำถามจากเอกสาร\n",
    "\n",
    "กติกา:\n",
    "- ใช้เฉพาะข้อมูลใน Context เท่านั้น\n",
    "- ต้องอ้างเลขหน้า\n",
    "- ถ้าไม่มีข้อมูล ให้ตอบว่า \"ไม่พบข้อมูลในเอกสาร\"\n",
    "- **ตอบด้วยภาษาเดียวกับภาษาของคำถามผู้ใช้**\n",
    "  - ถ้าคำถามเป็นภาษาไทย ให้ตอบภาษาไทย\n",
    "  - ถ้าคำถามเป็นภาษาอังกฤษ ให้ตอบภาษาอังกฤษ\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[Question]\n",
    "{question}\n",
    "\n",
    "[Answer]\n",
    "\"\"\"\n",
    "        return dspy.Prediction(answer=dspy.settings.lm(prompt))\n",
    "\n",
    "# ============================================================\n",
    "# 9) ORCHESTRATOR\n",
    "# ============================================================\n",
    "\n",
    "def ask_agent(question: str, chunks, index):\n",
    "    base = retrieve(question, chunks, index, BASE_TOP_K)\n",
    "\n",
    "    subs = QueryExpansionAgent()(question)\n",
    "    expanded = []\n",
    "    for q in subs:\n",
    "        expanded.extend(retrieve(q, chunks, index, EXPAND_TOP_K))\n",
    "\n",
    "    final = list({c[\"id\"]: c for c in base + expanded}.values())\n",
    "    if not final:\n",
    "        return \"ไม่พบข้อมูลในเอกสาร\"\n",
    "\n",
    "    context = \"\\n\\n---\\n\\n\".join(\n",
    "        f\"[หน้า {c['page']} | {c['section']}]\\n{c['text']}\"\n",
    "        for c in final[:6]\n",
    "    )\n",
    "\n",
    "    out = DocumentQAAgent()(context=context, question=question)\n",
    "    ans = out.answer.strip()\n",
    "    return ans if len(ans) > 10 else \"ไม่พบข้อมูลในเอกสาร\"\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t_start = time.perf_counter()\n",
    "    print(\"Agent #2: The Librarian\")\n",
    "\n",
    "    if not META_PATH.exists():\n",
    "        chunks = build_chunks()\n",
    "        save_chunks_jsonl(chunks)\n",
    "        save_markdown(chunks)\n",
    "    else:\n",
    "        chunks = load_chunks()\n",
    "\n",
    "    pdf_chars  = count_pdf_chars(PDF_PATH)\n",
    "    md_chars   = count_md_chars(MD_PATH)\n",
    "    json_chars = count_jsonl_chars(META_PATH)\n",
    "\n",
    "    report_char_count(pdf_chars, md_chars, json_chars)\n",
    "\n",
    "    t_embed_start = time.perf_counter()\n",
    "    embeddings = build_or_load_embeddings(chunks)\n",
    "    index = build_or_load_index(embeddings)\n",
    "    t_embed_end = time.perf_counter()\n",
    "\n",
    "    print(f\"⏱ Data preparation time : {t_embed_start - t_start:.2f} sec\")\n",
    "    print(f\"⏱ Embedding + Index time: {t_embed_end - t_embed_start:.2f} sec\")\n",
    "\n",
    "    print(\"\\nAgent Ready (type 'exit')\")\n",
    "\n",
    "    while True:\n",
    "        q = input(\"\\nQuestion: \")\n",
    "        if q.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        t_q = time.perf_counter()\n",
    "        ans = ask_agent(q, chunks, index)\n",
    "        t_q2 = time.perf_counter()\n",
    "\n",
    "        print(\"\\nAnswer:\\n\", ans)\n",
    "        print(f\"⏱ Query processing time : {t_q2 - t_q:.2f} sec\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
